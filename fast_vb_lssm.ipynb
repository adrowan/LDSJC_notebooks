{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Variational Bayesian Linear State-Space Model\n",
    "\n",
    "This notebook is based on the eponymous [paper](https://www.researchgate.net/profile/Jaakko_Luttinen/publication/256121415_Fast_Variational_Bayesian_Linear_State-Space_Model/links/02e7e535a2e9b467ad000000.pdf?origin=publication_list) by [Jaakko_Luttinen](https://www.researchgate.net/profile/Jaakko_Luttinen), the author of [BayesPy](http://bayespy.org/), and on the example in http://bayespy.org/en/latest/examples/lssm.html\n",
    "\n",
    "### Overview of Linear State Space Models\n",
    "\n",
    "We consider a class of models characterised by a noisy higher dimensional representation of some lower dimensional 'hidden' state. This can be applied to both static and dynamic (i.e. where ordering is important) datasets.  The basic models are discrete time linear dynamical systems with Gaussian noise. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear state-space models a sequence of $M$-dimensional observations\n",
    "$\\mathbf{Y}=(\\mathbf{y}_1,\\ldots,\\mathbf{y}_N)$ is assumed to be generated\n",
    "from latent $D$-dimensional states\n",
    "$\\mathbf{X}=(\\mathbf{x}_1,\\ldots,\\mathbf{x}_N)$ which follow a first-order\n",
    "Markov process:\n",
    "\n",
    "   \\begin{aligned}\n",
    "   \\mathbf{x}_{n} &= \\mathbf{A}\\mathbf{x}_{n-1} + \\mathbf{w}_{\\bullet}  \n",
    "   \\\\\n",
    "   \\mathbf{y}_{n} &= \\mathbf{C}\\mathbf{x}_{n} + \\mathbf{v}_{\\bullet} \n",
    "   \\end{aligned}\n",
    "\n",
    "$\\mathbf{A}$ is the $D\\times D$ state dynamics matrix and $\\mathbf{C}$ is the $M\\times D$ loading/observation matrix. Usually, the latent space dimensionality $D$ is assumed to be much smaller than the observation space dimensionality $M$ in order to model the dependencies of high-dimensional observations efficiently.\n",
    "\n",
    "Here the $D$-vector $\\mathbf{w}_{\\bullet} \\thicksim \\mathcal{N}(\\mathbf{0},\\mathbf{Q})$ and $M$-vector $\\mathbf{v}_{\\bullet} \\thicksim \\mathcal{N}(\\mathbf{0},\\mathbf{R})$ are random variables representing the state evolution and observations noises respectively. Both noise sources are uncorrelated from time step to time step and spatially Gaussian distributed with zero mean and covariance matrices $\\mathbf{Q}$ and $\\mathbf{R}$ respectively.  The noise processes do not have knowledge of the time index; hence we do not use a $t$ subscript for $\\mathbf{w}$ or $\\mathbf{v}$\n",
    "\n",
    "There is a useful unifying perspective on linear dynamical systems with Gaussian noise: we can actually consider PCA, Hidden Markov Models, Mixture of Gaussians and Kalman filtering to be limiting cases (see *\"A Unifying Review of Linear Gaussian Models\"*, S. Roweis, Z. Ghahramani, Neural Computation **11**, 305â€“345 (1999) )\n",
    "\n",
    "We can think intuitively about this as follows. At each time step the hidden noise term $\\mathbf{w}_{\\bullet}$ randomly generates a spherical ball of density (described by $\\mathbf{Q}$) in $k$-dimensional state space. In the dynamic case, this density flows around from time step to time step in the field described by the eigenvalues and eigenvectors of the matrix $\\mathbf{A}$. In the static case, each point in the dataset is generated independently and identically so the state dynamics matrix $\\mathbf{A}=\\mathbf{0}$ and the ball remains centred on the origin. \n",
    "\n",
    "In each case the ball is then stretched and rotated into a $p$-dimensional observation space by the matrix $\\mathbf{C}$, where it looks like a $k$-dimensional pancake. This pancake is then convolved with the observation noise to get the final convariance model for $\\mathbf{y}$.\n",
    "\n",
    "\n",
    "|             | Continuous States | Discrete States     |\n",
    "|------------ | -------------     | ---------------     |\n",
    "|**Dynamic**  | Kalman filter     | Hidden Markov Model |\n",
    "| **Static**  | Probabilistic PCA, SPCA, PCA, Factor Analysis$^{\\dagger}$ |  Gaussian Mixture  | \n",
    "$\\dagger$ *depending on choice of observation noise covariance matrix* $\\mathbf{R}$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear State Space Models with BayesPy\n",
    "\n",
    "In order to construct the model in [BayesPy](http://bayespy.org/), first import relevant nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "%matplotlib inline\n",
    "\n",
    "from bayespy.nodes import GaussianARD, GaussianMarkovChain, Gamma, Dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There will be 400 data vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let us use 10-dimensional latent space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state dynamics matrix $\\mathbf{A}$ has ARD prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = Gamma(1e-5,\n",
    "               1e-5,\n",
    "               plates=(D,),\n",
    "               name='alpha')\n",
    "A = GaussianARD(0,\n",
    "                 alpha,\n",
    "                 shape=(D,),\n",
    "                 plates=(D,),\n",
    "                 name='A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $\\mathbf{A}$ is a $D\\times{}D$-dimensional matrix.\n",
    "However, in BayesPy it is modelled as a collection (``plates=(D,)``) of\n",
    "$D$-dimensional vectors (``shape=(D,)``) because this is how the variables\n",
    "factorize in the posterior approximation of the state dynamics matrix in\n",
    "``GaussianMarkovChain``.  The latent states are constructed as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = GaussianMarkovChain(np.zeros(D),\n",
    "                         1e-3*np.identity(D),\n",
    "                         A,\n",
    "                         np.ones(D),\n",
    "                         n=N,\n",
    "                         name='X')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the first two arguments are the mean and precision matrix of the initial\n",
    "state, the third argument is the state dynamics matrix and the fourth argument\n",
    "is the diagonal elements of the precision matrix of the innovation noise.  The\n",
    "node also needs the length of the chain given as the keyword argument ``n=N``.\n",
    "Thus, the shape of this node is ``(N,D)``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear mapping from the latent space to the observation space is modelled\n",
    "with the loading matrix which has ARD prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = Gamma(1e-5,\n",
    "               1e-5,\n",
    "               plates=(D,),\n",
    "               name='gamma')\n",
    "C = GaussianARD(0,\n",
    "                 gamma,\n",
    "                 shape=(D,),\n",
    "                 plates=(M,1),\n",
    "                 name='C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the plates for ``C`` are ``(M,1)``, thus the full shape of the node is\n",
    "``(M,1,D)``.  The unit plate axis is added so that ``C`` broadcasts with ``X``\n",
    "when computing the dot product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F = Dot(C, X, name='F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dot product is computed over the $D$-dimensional latent space, thus\n",
    "the result is a $M\\times{}N$-dimensional matrix which is now represented\n",
    "with plates ``(M,N)`` in BayesPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F.plates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to use random initialization either for ``C`` or ``X`` in order to\n",
    "find non-zero latent space because by default both ``C`` and ``X`` are\n",
    "initialized to zero because of their prior distributions.  We use random\n",
    "initialization for ``C`` and then we must update ``X`` the first time before\n",
    "updating ``C``:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C.initialize_from_random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision of the observation noise is given gamma prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tau = Gamma(1e-5,\n",
    "             1e-5,\n",
    "             name='tau')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observations are noisy versions of the dot products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = GaussianARD(F,tau,name='Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variational Bayesian inference engine is then constructed as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bayespy.inference import VB\n",
    "Q = VB(X, C, gamma, A, alpha, tau, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ``X`` is given before ``C``, thus ``X`` is updated before ``C`` by\n",
    "default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "\n",
    "Now, let us generate some toy data for our model.  Our true latent space is four\n",
    "dimensional with two noisy oscillator components, one random walk component and\n",
    "one white noise component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = 0.3\n",
    "a = np.array([[np.cos(w), -np.sin(w), 0, 0], \n",
    "               [np.sin(w), np.cos(w),  0, 0], \n",
    "               [0,         0,          1, 0],\n",
    "               [0,         0,          0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true linear mapping is just random, i.e. we sample $\\mathbf{C}$ entries from a Gaussian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = np.random.randn(M,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, generate the latent states and the observations using the model equations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.empty((N,4))\n",
    "f = np.empty((M,N))\n",
    "y = np.empty((M,N))\n",
    "x[0] = 10*np.random.randn(4)\n",
    "f[:,0] = np.dot(c,x[0])\n",
    "y[:,0] = f[:,0] + 3*np.random.randn(M)\n",
    "for n in range(N-1):\n",
    "     x[n+1] = np.dot(a,x[n]) + [1,1,10,10]*np.random.randn(4)\n",
    "     f[:,n+1] = np.dot(c,x[n+1])\n",
    "     y[:,n+1] = f[:,n+1] + 3*np.random.randn(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to simulate missing values, thus we create a mask which randomly removes\n",
    "80% of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bayespy.utils import random\n",
    "mask = random.mask(M, N, p=0.2)\n",
    "Y.observe(y, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "As we did not define plotters for our nodes when creating the model, it is done\n",
    "now for some of the nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bayespy.plot as bpplt\n",
    "X.set_plotter(bpplt.FunctionPlotter(center=True, axis=-2))\n",
    "A.set_plotter(bpplt.HintonPlotter())\n",
    "C.set_plotter(bpplt.HintonPlotter())\n",
    "tau.set_plotter(bpplt.PDFPlotter(np.linspace(0.02, 0.5, num=1000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This enables plotting of the approximate posterior distributions during VB\n",
    "learning.  The inference engine can be run using ``VB.update()`` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q.update(repeat=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iteration progresses a bit slowly, thus we'll consider parameter expansion\n",
    "to speed it up.\n",
    "\n",
    "### Parameter expansion\n",
    "\n",
    "Section [Parameter Expansion](http://bayespy.org/en/latest/user_guide/inference.html#sec-parameter-expansion) discusses parameter expansion for state-space models to speed up inference.  It is based on a rotating the latent\n",
    "space such that the posterior in the observation space is not affected:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\mathbf{y}_n = \\mathbf{C}\\mathbf{x}_n = (\\mathbf{C}\\mathbf{R}^{-1}) (\\mathbf{R}\\mathbf{x}_n) \\,.$\n",
    "\n",
    "Thus, the transformation is\n",
    "$\\mathbf{C}\\rightarrow\\mathbf{C}\\mathbf{R}^{-1}$ and\n",
    "$\\mathbf{X}\\rightarrow\\mathbf{R}\\mathbf{X}$.  In order to keep the\n",
    "dynamics of the latent states unaffected by the transformation, the state\n",
    "dynamics matrix $\\mathbf{A}$ must be transformed accordingly:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{R}\\mathbf{x}_n = \\mathbf{R}\\mathbf{A}\\mathbf{R}^{-1}\n",
    "\\mathbf{R}\\mathbf{x}_{n-1} \\,,$\n",
    "\n",
    "resulting in a transformation $\\mathbf{A}\\rightarrow\\mathbf{R}\\mathbf{A}\\mathbf{R}^{-1}$.  For more\n",
    "details, refer to [Luttinen 2013](http://bayespy.org/en/latest/references.html#luttinen-2013) and [Luttinen 2010](http://bayespy.org/en/latest/references.html#luttinen-2010).  In BayesPy,\n",
    "the transformations are available in ``bayespy.inference.vmp.transformations``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bayespy.inference.vmp import transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rotation of the loading matrix along with the ARD parameters is defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rotC = transformations.RotateGaussianARD(C, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For rotating ``X``, we first need to define the rotation of the state dynamics\n",
    "matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rotA = transformations.RotateGaussianARD(A, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the rotation of the latent states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rotX = transformations.RotateGaussianMarkovChain(X, rotA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal rotation for all these variables is found using rotation optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R = transformations.RotationOptimizer(rotX, rotC, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameter expansion to be applied after each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q.callback = R.rotate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run iterations until convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q.update(repeat=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Because we have set the plotters, we can plot those nodes as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q.plot(X, A, C, tau)\n",
    "bpplt.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are clearly four effective components in ``X``: random walk (component\n",
    "number 1), random oscillation (6 and 10), and white noise (9).  These dynamics\n",
    "are also visible in the state dynamics matrix Hinton diagram.  Note that the\n",
    "white noise component does not have any dynamics.  Also ``C`` shows only four\n",
    "effective components.  The posterior of ``tau`` captures the true value\n",
    "$3^{-2}\\approx0.111$ accurately.  We can also plot predictions in the\n",
    "observation space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bpplt.plot(F, center=True)\n",
    "bpplt.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also measure the performance numerically by computing root-mean-square\n",
    "error (RMSE) of the missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bayespy.utils import misc\n",
    "misc.rmse(y[~mask], F.get_moments()[0][~mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is relatively close to the standard deviation of the noise (3), so the\n",
    "predictions are quite good considering that only 20% of the data was used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
